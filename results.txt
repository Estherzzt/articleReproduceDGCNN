(.venv) (base) zitongzhang@Zitongs-MacBook-Air-5 pytorch % python main.py --exp_name=dgcnn_1024 --model=dgcnn --num_points=1024 --k=20 --use_sgd=True --batch_size=32 --epochs=250
Namespace(exp_name='dgcnn_1024', model='dgcnn', dataset='modelnet40', batch_size=32, test_batch_size=16, epochs=250, use_sgd=True, lr=0.001, momentum=0.9, no_cuda=False, seed=1, eval=False, num_points=1024, dropout=0.5, emb_dims=1024, k=20, model_path='')
Using CPU
DGCNN(
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv1): Sequential(
    (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2)
  )
  (conv2): Sequential(
    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2)
  )
  (conv3): Sequential(
    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2)
  )
  (conv4): Sequential(
    (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2)
  )
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): LeakyReLU(negative_slope=0.2)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=True)
  (bn7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
Let's use 0 GPUs!
Use SGD
/Users/zitongzhang/Downloads/dgcnn-master/pytorch/.venv/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
Train 0, loss: 2.801006, train acc: 0.427932, train avg acc: 0.253428
Test 0, loss: 2.225270, test acc: 0.634522, test avg acc: 0.486401
Train 1, loss: 2.212991, train acc: 0.628970, train avg acc: 0.449799
Test 1, loss: 2.038298, test acc: 0.703404, test avg acc: 0.571250
Train 2, loss: 2.058173, train acc: 0.696763, train avg acc: 0.534226
Test 2, loss: 1.943155, test acc: 0.751621, test avg acc: 0.630238
Train 3, loss: 1.950371, train acc: 0.749898, train avg acc: 0.610410
Test 3, loss: 1.876912, test acc: 0.775122, test avg acc: 0.679029
Train 4, loss: 1.898481, train acc: 0.770664, train avg acc: 0.646166
Test 4, loss: 1.778093, test acc: 0.812804, test avg acc: 0.710465
Train 5, loss: 1.853989, train acc: 0.790004, train avg acc: 0.675896
Test 5, loss: 1.711324, test acc: 0.848865, test avg acc: 0.782738
Train 6, loss: 1.813590, train acc: 0.807410, train avg acc: 0.703426
Test 6, loss: 1.794308, test acc: 0.804295, test avg acc: 0.706366
Train 7, loss: 1.795324, train acc: 0.815350, train avg acc: 0.712901
Test 7, loss: 1.721501, test acc: 0.821718, test avg acc: 0.709436
Train 8, loss: 1.769061, train acc: 0.819320, train avg acc: 0.717935
Test 8, loss: 1.669531, test acc: 0.853323, test avg acc: 0.770587
Train 9, loss: 1.751483, train acc: 0.830212, train avg acc: 0.733038
Test 9, loss: 1.685236, test acc: 0.842788, test avg acc: 0.778250
Train 10, loss: 1.732432, train acc: 0.840798, train avg acc: 0.748288
Test 10, loss: 1.609076, test acc: 0.880065, test avg acc: 0.801965
Train 11, loss: 1.721708, train acc: 0.843445, train avg acc: 0.760574
Test 11, loss: 1.634425, test acc: 0.859806, test avg acc: 0.806977
Train 12, loss: 1.701066, train acc: 0.855354, train avg acc: 0.773406
Test 12, loss: 1.631290, test acc: 0.865883, test avg acc: 0.802605
Train 13, loss: 1.699907, train acc: 0.851792, train avg acc: 0.769444
Test 13, loss: 1.626839, test acc: 0.860616, test avg acc: 0.801052
Train 14, loss: 1.692979, train acc: 0.856576, train avg acc: 0.778200
Test 14, loss: 1.637381, test acc: 0.864263, test avg acc: 0.792169
Train 15, loss: 1.675120, train acc: 0.864108, train avg acc: 0.789867
Test 15, loss: 1.613617, test acc: 0.873177, test avg acc: 0.797797
Train 16, loss: 1.670962, train acc: 0.867569, train avg acc: 0.792846
Test 16, loss: 1.624762, test acc: 0.875203, test avg acc: 0.802052
Train 17, loss: 1.665795, train acc: 0.866246, train avg acc: 0.789521
Test 17, loss: 1.599096, test acc: 0.879660, test avg acc: 0.825215
Train 18, loss: 1.652976, train acc: 0.874186, train avg acc: 0.803523
Test 18, loss: 1.599190, test acc: 0.887763, test avg acc: 0.828128
